from bs4 import BeautifulSoup
from bs4.element import Tag  # scrape
import requests
import re
import datetime


class Scrape:
    def __init__(self, file):
        self.url = "https://www.boatrace.jp/owpc/pc/data/racersearch/profile?toban="
        self.f_name = file

    def crawling(self):
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) \
        AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15'}
        url = self.url + self.f_name
        response = requests.get(url, headers=headers)
        response.encoding = response.apparent_encoding
        with open(self.f_name, 'w', encoding='utf-8') as f:
            f.write(response.text)
        """
        try:
            with open(self.f_name, 'w', encoding="utf-8") as f:
                f.write(response.text)
        except FileNotFoundError:
            print("File Not Found")
        """

    def scraping(self):
        page = self.f_name
        data = []
        data_p = []
        grade_check = {"is-ippan":"ä¸€èˆ¬", "is-G3b":"G3", "is-G2b":"G2", "is-G1b":"G1", "is-SGa":"SG"}
        grade_another = {"is-venus":"ãƒ´ã‚£ãƒ¼ãƒŠã‚¹", "is-rookie__3rdadd":"ãƒ«ãƒ¼ã‚­ãƒ¼", "is-lady":"ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹"}
        time_check = {"is-nighter": "ğŸŒ˜", "is-morning": "â˜€"}
        re_space = re.compile(r"\s+")
        with open(page, 'r') as html:  # å–å¾—éƒ¨
            soup = BeautifulSoup(html, 'html.parser')
            race_data = soup.find_all(class_='is-p10-0')    #å‡ºèµ°ãƒ‡ãƒ¼ã‚¿å…¨ã¦å–å¾—
            racer_name = soup.find(class_='racer1_bodyName').text
            racer_name = re_space.sub(" ", racer_name)  #ãƒ¬ãƒ¼ã‚µãƒ¼åã®ã¿å–å¾—
        for d in race_data:
            if d == "": continue
            place = str(d.find("img")) #imgã‚¿ã‚°å†…ã®é–‹å‚¬åœ°åã®æŠœãå‡ºã—
            place = re_space.sub("", place[9:14])
            place = place.replace("\"","")    #é–‹å‚¬åœ°åã®ã¿
            grade = ""
            for c in grade_check.keys():   #ã‚°ãƒ¬ãƒ¼ãƒ‰ã®æŠœãå‡ºã—
                if not d.find(class_=c) :continue
                grade_tag = str(d.find(class_=c)).replace("<td class=\"is-p10-5 ", "")\
                    .replace("\"></td>", "").split(" ") #ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒã‚ã‹ã‚‹ã‚¿ã‚°ã‚’æŠœãå‡ºã—
                grade = grade_check[c]
                if len(grade_tag) > 1:  #ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹å ´åˆ
                    grade += grade_another[grade_tag[1]]
                break
            venue = f"{place}({grade})"     #ãƒ¢ãƒ¼ãƒ‹ãƒ³ã‚°orãƒŠã‚¤ã‚¿ãƒ¼ã®å ´åˆã€è¡¨è¨˜è¿½åŠ 
            for t in time_check.keys():
                time_tag = str(d.find(class_=t)).replace("<td class=\"", "")\
                    .replace("\"></td>", "").split(" ")
                if time_tag[0] == t:
                    venue = f"{place}{time_check[t]}({grade})"
            d = re_space.sub("", d.text)
            data.append(d)
            data_p.append(venue)
            #return racer_name, [data_p, data]  # é–‹å‚¬åœ°+ãƒ‡ãƒ¼ã‚¿
        return racer_name,[data_p, data]  # é–‹å‚¬åœ°+ãƒ‡ãƒ¼ã‚¿

def generator(data):
    for d in data:
        yield d

def check(name,data,time):
    p = ""
    re_ZenHan = re.compile(r"[ï¸°-ï¼ ä¸€-é¾¥ï¼¡-ï¼ºï½-ï½šã-ã‚“ã‚¡-ãƒ³ãƒ¼]")
    re_num = re.compile(r"[0-9Rï½/\-]")
    data_gen = generator(data[1])
    #print(data_gen.__next__())
    #print(j.__next__())
    try:
        d = data_gen.__next__()
        if d[:10] == time:
            r_num = re_ZenHan.sub("", d[10:]).split("R")  # ãƒ¬ãƒ¼ã‚¹ç•ªå·æŠ½å‡ºã€åŠè§’è‹±æ•°ä»¥å¤–ã‚’æ¶ˆã™
            r_name = re_num.sub("", d[10:])    #ãƒ¬ãƒ¼ã‚¹åã®ã¿
            return f"â˜†{name}â˜†\nâ˜…{data[0][0]}â˜…ã®\"{r_name}\"\n{r_num[0]}Rã¨{r_num[1]}Rã«å‡ºã‚‹ã‚ˆ."
        else:
            #d = data_gen.__next__()
            r_name = re_num.sub("", d[10:])  # ãƒ¬ãƒ¼ã‚¹åã®ã¿
            return f"â–³{name}ã¯æœ¬æ—¥å‡ºå ´äºˆå®šç„¡ã—.\næ¬¡ç¯€ã¯, {d[:21]}, â˜…{data[0][0]}â˜…ã®\"{r_name}\"ã«å‡ºå ´äºˆå®š"
    #except IndexError:
    except :
        return f"Ã—{name}ã¯æ–¡æ—‹æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ–¡æ—‹æƒ…å ±ãŒç„¡ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
    """
    for d in data[1]:
        #print(name,d)
        try:    #ã‚µã‚¤ãƒˆæ›´æ–°ãªã„å ´åˆã‚¨ãƒ©ãƒ¼ã«ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š
            if d[:10]==time:
                r = re.sub(r"[ï¸°-ï¼ ä¸€-é¾¥ï¼¡-ï¼ºï½-ï½šã-ã‚“ã‚¡-ãƒ³ãƒ¼]", "", d[10:]).split("R") #ãƒ¬ãƒ¼ã‚¹ç•ªå·æŠ½å‡ºã€åŠè§’è‹±æ•°ä»¥å¤–ã‚’æ¶ˆã™
                d = re.sub(r"[0-9R\-]", "", d[10:])
                p = f"â˜†{name}â˜†\n{data[0][i]}ã®{d}\n{r[0]}Rã¨{r[1]}Rã«å‡ºã‚‹ã‚ˆ"
                return p
            else:
                p = f"{name}ã¯å‡ºå ´äºˆå®šç„¡ã—"
        except IndexError:
            p = f"[{name}]å–å¾—ã‚¨ãƒ©ãƒ¼"
    """

    return p     

class Line_notify:
    def __init__(self, data):
        with open("LINE_access_token","r")as f:
            token = f.read()
        self.access_token = token
        self.data = data

    def send_message(self):
        url = "https://notify-api.line.me/api/notify"
        headers = {"Authorization": "Bearer " + self.access_token}

        message = f"\n{self.data}"
        payload = {"message":  message}

        r = requests.post(url, headers=headers, params=payload)


if __name__ == "__main__":
    todays_boat = "https://www.boatrace.jp/owpc/pc/race/index"
    favo_num = ["3188", "3992", "4330", "4556", "4737", "4969", "4947", "5012", "5028"]
    jptime = datetime.datetime.now(datetime.timezone(datetime.timedelta(hours=9)))  # UTC+9ã§æ—¥æœ¬æ™‚åˆ»
    jptime = jptime.strftime('%Y/%m/%d %H:%M:%S').split(" ")    #ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆã‚ã›+æ™‚åˆ»ã¨åˆ†ã‘ã‚‹
    tmp = []
    p_now = ""
    p_next = ""
    p_error = ""
    for num in favo_num:
        scrape = Scrape(num)
        scrape.crawling()
        name,data = scrape.scraping()
        tmp.append(check(name,data,jptime[0]))

    for p in tmp:   #ãã®æ—¥ã«èµ°ã‚‹é¸æ‰‹ã‚’å…ˆã«è¡¨ç¤º
        if p[0] == "â˜†":
            p_now += p + "\n\n"
        elif p[0] == "Ã—":
            p_error += p + "\n\n"
        else:
            p_next += p + "\n\n"

    message = p_now + p_next + p_error + todays_boat

    Line_notify(message).send_message()
    #print(message)
